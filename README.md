# LLM Benchmark para DetecÃ§Ã£o de Fake News em PortuguÃªs

Este projeto implementa um benchmark comparativo de Large Language Models (LLMs) locais para a tarefa de detecÃ§Ã£o de fake news em portuguÃªs brasileiro.

## ğŸ“‹ VisÃ£o Geral

O benchmark avalia diferentes modelos de linguagem usando as seguintes estratÃ©gias:
- **Zero-Shot**: ClassificaÃ§Ã£o sem exemplos prÃ©vios
- **Few-Shot**: ClassificaÃ§Ã£o com 3 exemplos demonstrativos
- **Chain-of-Thought**: ClassificaÃ§Ã£o com raciocÃ­nio passo a passo
- **Fine-tuned**: BERT em portuguÃªs fine-tunado nos datasets (para comparaÃ§Ã£o com os LLMs)

## ğŸš€ InstalaÃ§Ã£o

### 1. Criar ambiente virtual

```bash
# Com venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Com conda
conda create -n llm-benchmark python=3.13
conda activate llm-benchmark
```

### 2. Instalar dependÃªncias

O projeto usa [uv](https://github.com/astral-sh/uv) e `pyproject.toml` (Python â‰¥3.13).

```bash
# Com uv (recomendado)
uv sync

# Ou com pip
pip install -e .
```

### 3. Instalar PyTorch com CUDA (para GPU)

Se for usar modelos Hugging Face locais com GPU:

```bash
# CUDA 12.1
pip install torch --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ“Š Modelos IncluÃ­dos

| Categoria | Modelo (Hugging Face) | ParÃ¢metros | VRAM Estimada |
|-----------|------------------------|------------|---------------|
| Pequeno | Qwen/Qwen2-1.5B-Instruct | 1.5B | ~4 GB |
| Pequeno | CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it | 4B | ~6 GB |
| Pequeno | maritaca-ai/sabia-7b | 7B | ~8 GB |
| MÃ©dio | Qwen/Qwen2-7B-Instruct | 7B | ~8 GB |
| MÃ©dio | meta-llama/Meta-Llama-3-8B-Instruct | 8B | ~10 GB |
| MÃ©dio | lucianosb/boto-9B-it | 9B | ~12 GB |

*Valores com quantizaÃ§Ã£o 4-bit*

## ğŸ“ Datasets

O benchmark utiliza dois datasets de fake news em portuguÃªs:

1. **Fake.Br Corpus** (7.200 notÃ­cias)
   - Fonte: https://github.com/roneysco/Fake.br-Corpus

2. **FakeRecogna** (11.902 notÃ­cias)
   - Fonte: https://github.com/Gabriel-Lino-Garcia/FakeRecogna

Os datasets sÃ£o baixados automaticamente do Hugging Face.

## ğŸ¯ Uso

O fluxo de trabalho Ã© baseado em **Jupyter notebooks**. Execute os notebooks a partir da raiz do projeto (`tcc2/`); eles configuram o `sys.path` para importar os mÃ³dulos em `src/`.

### Fluxo recomendado

1. **`01_data_exploration.ipynb`** â€” Carrega e explora os datasets (Fake.Br e FakeRecogna).
2. **`02_single_model_test.ipynb`** â€” Testa um Ãºnico modelo/estratÃ©gia antes do benchmark completo.
3. **`03_full_benchmark.ipynb`** â€” Executa o benchmark completo (todos os modelos Ã— estratÃ©gias Ã— datasets). Pode levar vÃ¡rias horas. Os resultados sÃ£o salvos em `reports/*.json`.
4. **`03_train_bert.ipynb`** â€” Fine-tuning do BERT (neuralmind/bert-base-portuguese-cased) para comparaÃ§Ã£o com os LLMs.
5. **`04_benchmark_analysis.ipynb`** â€” Agrega os JSONs, gera tabelas e o relatÃ³rio em `reports/benchmark_report.*`.

### ConfiguraÃ§Ãµes

Modelos, datasets, estratÃ©gias e parÃ¢metros de experimento estÃ£o em **`src/config.py`** (`MODELS`, `DATASETS`, `PROMPTING_STRATEGIES`, `EXPERIMENT_CONFIG`). Os templates de prompt ficam em **`src/models/prompts.py`**.

## ğŸ“ˆ MÃ©tricas Avaliadas

### MÃ©tricas de Desempenho
- **AcurÃ¡cia**: Percentual de classificaÃ§Ãµes corretas
- **PrecisÃ£o**: Taxa de verdadeiros positivos entre os preditos como fake
- **Recall**: Taxa de detecÃ§Ã£o de fake news
- **F1-Score**: MÃ©dia harmÃ´nica entre precisÃ£o e recall

### MÃ©tricas PrÃ¡ticas
- **Tempo de InferÃªncia**: Segundos por notÃ­cia
- **Uso de VRAM**: MemÃ³ria de vÃ­deo utilizada

## ğŸ“‚ Estrutura do Projeto

```
tcc2/
â”œâ”€â”€ main.py                 # Ponto de entrada (stub)
â”œâ”€â”€ pyproject.toml          # DependÃªncias e metadados
â”œâ”€â”€ uv.lock                 # Lock file (uv)
â”œâ”€â”€ README.md               # Este arquivo
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # Modelos, datasets, estratÃ©gias, paths
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_loader.py  # Carregamento de dados (Hugging Face)
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ model_handler.py  # ModelHandler (HF) e ModelHandlerOllama
â”‚       â”œâ”€â”€ prompts.py       # Templates de prompts
â”‚       â””â”€â”€ metrics.py       # CÃ¡lculo de mÃ©tricas e relatÃ³rio
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_single_model_test.ipynb
â”‚   â”œâ”€â”€ 03_full_benchmark.ipynb
â”‚   â”œâ”€â”€ 03_train_bert.ipynb   # Fine-tuning BERT
â”‚   â””â”€â”€ 04_benchmark_analysis.ipynb
â”œâ”€â”€ reports/                # Resultados (gerados pelos notebooks)
â”‚   â”œâ”€â”€ benchmark_report.csv
â”‚   â”œâ”€â”€ benchmark_report.xlsx
â”‚   â”œâ”€â”€ benchmark_report.md
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ *.json               # Resultados por modelo/estratÃ©gia/dataset
â”œâ”€â”€ models/                  # Checkpoints locais (ex.: BERT)
â””â”€â”€ references/              # ReferÃªncias
```

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Modificar modelos

Edite **`src/config.py`** para adicionar ou remover modelos e o mapeamento para Ollama:

```python
MODELS = [
    "seu-modelo/nome",
    # ...
]

OLLAMA_MODEL_MAPPING = {
    "huggingface/model-name": "ollama:model-name",
    # ...
}
```

### Modificar prompts

Edite **`src/models/prompts.py`** para customizar os templates (zero-shot, few-shot, chain-of-thought).

### Usar Ollama

O benchmark usa **Ollama** por padrÃ£o nos notebooks (via `ModelHandlerOllama`). Certifique-se de que o Ollama estÃ¡ instalado e que os modelos listados em `OLLAMA_MODEL_MAPPING` estÃ£o disponÃ­veis. Nos notebooks, apÃ³s configurar `sys.path` com `src/`:

```python
from models.model_handler import ModelHandlerOllama

# Nome do modelo no Hugging Face; o handler usa OLLAMA_MODEL_MAPPING
handler = ModelHandlerOllama("meta-llama/Meta-Llama-3-8B-Instruct")
response = handler.generate(prompt)
```

## âš ï¸ Requisitos de Hardware

| ConfiguraÃ§Ã£o | VRAM | Modelos Suportados |
|--------------|------|-------------------|
| MÃ­nima | 8 GB | Qwen2-1.5B, Gemma-4B |
| Recomendada | 16 GB | Todos atÃ© 8B |
| Ideal | 24 GB | Todos os modelos |

## ğŸ“ CitaÃ§Ã£o

Se utilizar este cÃ³digo em sua pesquisa, por favor cite:

```bibtex
@misc{llm_fake_news_benchmark,
  author = {JoÃ£o},
  title = {LLM Benchmark para DetecÃ§Ã£o de Fake News em PortuguÃªs},
  year = {2026},
  publisher = {GitHub},
  url = {https://github.com/joao-roldi/tcc_llm_bert_benchmark}
}
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, abra uma issue ou pull request.
