{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Benchmark Completo\n",
        "\n",
        "Este notebook executa o benchmark completo comparando m√∫ltiplos modelos LLM.\n",
        "\n",
        "## Objetivos\n",
        "1. Testar todos os modelos configurados\n",
        "2. Comparar todas as estrat√©gias de prompting\n",
        "3. Avaliar em m√∫ltiplos datasets\n",
        "4. Salvar resultados em arquivos JSON\n",
        "\n",
        "‚ö†Ô∏è **Aten√ß√£o**: Este notebook pode levar v√°rias horas para executar!\n",
        "\n",
        "üìä **An√°lise**: Ap√≥s a execu√ß√£o, use o notebook `04_benchmark_analysis.ipynb` para analisar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 21:47:49.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mProject root: /Users/joaoroldi/Projects/tcc2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Configurar path do projeto\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Configurar logger\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\")\n",
        "logger.add(project_root / \"reports/logs/03_full_benchmark.log\", rotation=\"10 MB\")\n",
        "\n",
        "logger.info(f\"Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 21:47:52.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1m‚úÖ Ollama conectado. Modelos dispon√≠veis: 6\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m   - llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m   - qwen2:7b-instruct\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m   - hf.co/TheBloke/sabia-7B-GGUF:latest\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m   - brunoconterato/Gemma-3-Gaia-PT-BR-4b-it:f16\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m   - qwen2:1.5b-instruct\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m   - llama2:latest\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# M√≥dulos do projeto\n",
        "from config import MODELS, PROMPTING_STRATEGIES, DATASETS, EXPERIMENT_CONFIG, PATHS\n",
        "from data.data_loader import load_dataset, prepare_test_set\n",
        "from models.model_handler import ModelHandlerOllama\n",
        "from models.prompts import PromptBuilder\n",
        "from models.metrics import calculate_metrics, generate_report\n",
        "\n",
        "# Verificar Ollama\n",
        "try:\n",
        "    import ollama\n",
        "    ollama_models = ollama.list()\n",
        "    # Ollama retorna ListResponse com .models (n√£o dict com 'models')\n",
        "    models_list = ollama_models.models if hasattr(ollama_models, 'models') else []\n",
        "    logger.info(f\"‚úÖ Ollama conectado. Modelos dispon√≠veis: {len(models_list)}\")\n",
        "    for model in models_list:\n",
        "        # Cada modelo √© um objeto com atributo .model (n√£o dict com 'name')\n",
        "        model_name = model.model if hasattr(model, 'model') else str(model)\n",
        "        logger.info(f\"   - {model_name}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"‚ùå Erro ao conectar com Ollama: {e}\")\n",
        "    logger.error(\"   Certifique-se de que Ollama est√° instalado e rodando.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configura√ß√£o do Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 21:47:52.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mModelos: 6\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mEstrat√©gias: 3\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mDatasets: 2\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mTotal de experimentos: 36\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Configura√ß√µes\n",
        "SAMPLE_SIZE = 1000  # N√∫mero de amostras por dataset\n",
        "SEED = 42\n",
        "\n",
        "# Modelos a testar (pode reduzir para teste r√°pido)\n",
        "MODELS_TO_TEST = MODELS  # Descomentar para testar todos\n",
        "\n",
        "# Estrat√©gias a testar\n",
        "STRATEGIES_TO_TEST = PROMPTING_STRATEGIES\n",
        "\n",
        "# Datasets a testar\n",
        "DATASETS_TO_TEST = list(DATASETS.keys())\n",
        "\n",
        "logger.info(f\"Modelos: {len(MODELS_TO_TEST)}\")\n",
        "logger.info(f\"Estrat√©gias: {len(STRATEGIES_TO_TEST)}\")\n",
        "logger.info(f\"Datasets: {len(DATASETS_TO_TEST)}\")\n",
        "logger.info(f\"Total de experimentos: {len(MODELS_TO_TEST) * len(STRATEGIES_TO_TEST) * len(DATASETS_TO_TEST)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fun√ß√£o de Execu√ß√£o de Experimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_prediction(response: str) -> int:\n",
        "    \"\"\"Extrai predi√ß√£o da resposta do modelo.\"\"\"\n",
        "    response_lower = response.lower().strip()\n",
        "    \n",
        "    fake_keywords = [\"falsa\", \"fake\", \"falso\", \"mentira\", \"desinforma√ß√£o\", \"1\"]\n",
        "    true_keywords = [\"verdadeira\", \"verdadeiro\", \"real\", \"ver√≠dica\", \"0\"]\n",
        "    \n",
        "    for keyword in fake_keywords:\n",
        "        if keyword in response_lower:\n",
        "            return 1\n",
        "    \n",
        "    for keyword in true_keywords:\n",
        "        if keyword in response_lower:\n",
        "            return 0\n",
        "    \n",
        "    return -1\n",
        "\n",
        "\n",
        "def run_experiment(model_name, strategy, dataset_name, sample_size, seed):\n",
        "    \"\"\"Executa um √∫nico experimento.\"\"\"\n",
        "    logger.info(f\"Experimento: {model_name} | {strategy} | {dataset_name}\")\n",
        "    \n",
        "    try:\n",
        "        # 1. Carregar dados\n",
        "        logger.info(\"[1/4] Carregando dataset...\")\n",
        "        df = load_dataset(dataset_name)\n",
        "        test_df = prepare_test_set(df, sample_size=sample_size, seed=seed)\n",
        "        logger.info(f\"      Amostras: {len(test_df)}\")\n",
        "        \n",
        "        # 2. Carregar modelo (Ollama)\n",
        "        logger.info(\"[2/4] Carregando modelo via Ollama...\")\n",
        "        model_handler = ModelHandlerOllama(model_name)\n",
        "        vram_usage = model_handler.get_vram_usage()\n",
        "        logger.info(f\"      Modelo Ollama: {model_handler.ollama_model}\")\n",
        "        logger.info(f\"      Mem√≥ria estimada: {vram_usage:.2f} GB\")\n",
        "        \n",
        "        # 3. Preparar prompts\n",
        "        logger.info(\"[3/4] Preparando prompts...\")\n",
        "        prompt_builder = PromptBuilder(strategy)\n",
        "        \n",
        "        if strategy == \"few_shot\":\n",
        "            examples_df = df[~df.index.isin(test_df.index)].sample(\n",
        "                n=EXPERIMENT_CONFIG[\"few_shot_examples\"],\n",
        "                random_state=seed\n",
        "            )\n",
        "            prompt_builder.set_examples(examples_df)\n",
        "        \n",
        "        # 4. Executar infer√™ncia\n",
        "        logger.info(\"[4/4] Executando infer√™ncia...\")\n",
        "        predictions = []\n",
        "        inference_times = []\n",
        "        \n",
        "        for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"      \"):\n",
        "            prompt = prompt_builder.build_prompt(row[\"text\"])\n",
        "            \n",
        "            start_time = time.time()\n",
        "            response = model_handler.generate(prompt)\n",
        "            end_time = time.time()\n",
        "            \n",
        "            prediction = extract_prediction(response)\n",
        "            predictions.append(prediction)\n",
        "            inference_times.append(end_time - start_time)\n",
        "        \n",
        "        # Calcular m√©tricas\n",
        "        true_labels = test_df[\"label\"].tolist()\n",
        "        metrics = calculate_metrics(true_labels, predictions)\n",
        "        \n",
        "        metrics[\"avg_inference_time\"] = sum(inference_times) / len(inference_times)\n",
        "        metrics[\"total_inference_time\"] = sum(inference_times)\n",
        "        metrics[\"vram_usage_gb\"] = vram_usage\n",
        "        \n",
        "        # Resultados\n",
        "        results = {\n",
        "            \"model\": model_name,\n",
        "            \"strategy\": strategy,\n",
        "            \"dataset\": dataset_name,\n",
        "            \"sample_size\": sample_size,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"metrics\": metrics,\n",
        "        }\n",
        "        \n",
        "        # Liberar mem√≥ria (Ollama gerencia automaticamente)\n",
        "        model_handler.unload()\n",
        "        \n",
        "        logger.info(f\"‚úÖ Conclu√≠do! F1-Score: {metrics['f1_score']:.4f}\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå ERRO: {e}\")\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"strategy\": strategy,\n",
        "            \"dataset\": dataset_name,\n",
        "            \"error\": str(e)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Executar Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 21:47:52.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mIniciando benchmark com 36 experimentos...\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 1/36] Pular: Qwen/Qwen2-1.5B-Instruct | zero_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 2/36] Pular: Qwen/Qwen2-1.5B-Instruct | zero_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 3/36] Pular: Qwen/Qwen2-1.5B-Instruct | few_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 4/36] Pular: Qwen/Qwen2-1.5B-Instruct | few_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 5/36] Pular: Qwen/Qwen2-1.5B-Instruct | chain_of_thought | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 6/36] Pular: Qwen/Qwen2-1.5B-Instruct | chain_of_thought | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 7/36] Pular: CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it | zero_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 8/36] Pular: CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it | zero_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 9/36] Pular: CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it | few_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 10/36] Pular: CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it | few_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 11/36] Pular: CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it | chain_of_thought | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 12/36] Pular: CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it | chain_of_thought | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 13/36] Pular: maritaca-ai/sabia-7b | zero_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 14/36] Pular: maritaca-ai/sabia-7b | zero_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 15/36] Pular: maritaca-ai/sabia-7b | few_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 16/36] Pular: maritaca-ai/sabia-7b | few_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 17/36] Pular: maritaca-ai/sabia-7b | chain_of_thought | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 18/36] Pular: maritaca-ai/sabia-7b | chain_of_thought | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 19/36] Pular: Qwen/Qwen2-7B-Instruct | zero_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 20/36] Pular: Qwen/Qwen2-7B-Instruct | zero_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 21/36] Pular: Qwen/Qwen2-7B-Instruct | few_shot | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 22/36] Pular: Qwen/Qwen2-7B-Instruct | few_shot | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 23/36] Pular: Qwen/Qwen2-7B-Instruct | chain_of_thought | fakebr (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1m[Experimento 24/36] Pular: Qwen/Qwen2-7B-Instruct | chain_of_thought | fakerecogna (J√° conclu√≠do)\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 25/36] Re-executando (erro anterior): meta-llama/Meta-Llama-3-8B-Instruct | zero_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 25/36] Executando: meta-llama/Meta-Llama-3-8B-Instruct | zero_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: meta-llama/Meta-Llama-3-8B-Instruct | zero_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:52.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: Fake.Br Corpus\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 7200 amostras\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.model_handler\u001b[0m:\u001b[36m_check_ollama\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1mOllama conectado. Usando modelo: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m      Modelo Ollama: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m      Mem√≥ria estimada: 2.00 GB\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1m[3/4] Preparando prompts...\u001b[0m\n",
            "\u001b[32m2026-01-25 21:47:56.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m[4/4] Executando infer√™ncia...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6bfc833d83d49468d61de5bbbfe6df7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "      :   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 22:13:24.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1m‚úÖ Conclu√≠do! F1-Score: 0.0215\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:24.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 26/36] Re-executando (erro anterior): meta-llama/Meta-Llama-3-8B-Instruct | zero_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:24.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 26/36] Executando: meta-llama/Meta-Llama-3-8B-Instruct | zero_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:24.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: meta-llama/Meta-Llama-3-8B-Instruct | zero_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:24.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:24.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: FakeRecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 52800 amostras\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.model_handler\u001b[0m:\u001b[36m_check_ollama\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1mOllama conectado. Usando modelo: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m      Modelo Ollama: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m      Mem√≥ria estimada: 2.00 GB\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1m[3/4] Preparando prompts...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:13:27.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m[4/4] Executando infer√™ncia...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "693ea39552904ed091210b2cc37526a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "      :   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 22:25:19.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1m‚úÖ Conclu√≠do! F1-Score: 0.6999\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:19.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 27/36] Re-executando (erro anterior): meta-llama/Meta-Llama-3-8B-Instruct | few_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:19.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 27/36] Executando: meta-llama/Meta-Llama-3-8B-Instruct | few_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:19.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: meta-llama/Meta-Llama-3-8B-Instruct | few_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:19.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:19.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: Fake.Br Corpus\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 7200 amostras\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.model_handler\u001b[0m:\u001b[36m_check_ollama\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1mOllama conectado. Usando modelo: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m      Modelo Ollama: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m      Mem√≥ria estimada: 2.00 GB\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1m[3/4] Preparando prompts...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:25:21.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m[4/4] Executando infer√™ncia...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eec507bd27344129d2de627894d0d3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "      :   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 22:50:50.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1m‚úÖ Conclu√≠do! F1-Score: 0.0540\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:50.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 28/36] Re-executando (erro anterior): meta-llama/Meta-Llama-3-8B-Instruct | few_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:50.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 28/36] Executando: meta-llama/Meta-Llama-3-8B-Instruct | few_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:50.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: meta-llama/Meta-Llama-3-8B-Instruct | few_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:50.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:50.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: FakeRecogna\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 52800 amostras\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.model_handler\u001b[0m:\u001b[36m_check_ollama\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1mOllama conectado. Usando modelo: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m      Modelo Ollama: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m      Mem√≥ria estimada: 2.00 GB\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1m[3/4] Preparando prompts...\u001b[0m\n",
            "\u001b[32m2026-01-25 22:50:54.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m[4/4] Executando infer√™ncia...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9fc2fc55ea4fb4b2000098142c2ce0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "      :   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-25 23:02:47.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1m‚úÖ Conclu√≠do! F1-Score: 0.8697\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:47.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 29/36] Re-executando (erro anterior): meta-llama/Meta-Llama-3-8B-Instruct | chain_of_thought | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:47.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 29/36] Executando: meta-llama/Meta-Llama-3-8B-Instruct | chain_of_thought | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:47.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: meta-llama/Meta-Llama-3-8B-Instruct | chain_of_thought | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:47.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:47.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: Fake.Br Corpus\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 7200 amostras\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.model_handler\u001b[0m:\u001b[36m_check_ollama\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1mOllama conectado. Usando modelo: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m      Modelo Ollama: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m      Mem√≥ria estimada: 2.00 GB\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1m[3/4] Preparando prompts...\u001b[0m\n",
            "\u001b[32m2026-01-25 23:02:48.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m[4/4] Executando infer√™ncia...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8fab1d3ac5b46fc80f081f8f526cffe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "      :   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-26 00:30:18.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1m‚úÖ Conclu√≠do! F1-Score: 0.2342\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:18.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 30/36] Re-executando (erro anterior): meta-llama/Meta-Llama-3-8B-Instruct | chain_of_thought | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:18.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 30/36] Executando: meta-llama/Meta-Llama-3-8B-Instruct | chain_of_thought | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:18.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: meta-llama/Meta-Llama-3-8B-Instruct | chain_of_thought | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:18.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:18.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: FakeRecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 52800 amostras\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.model_handler\u001b[0m:\u001b[36m_check_ollama\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1mOllama conectado. Usando modelo: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m      Modelo Ollama: llama3:8b-instruct-q8_0\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1m      Mem√≥ria estimada: 2.00 GB\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1m[3/4] Preparando prompts...\u001b[0m\n",
            "\u001b[32m2026-01-26 00:30:20.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m[4/4] Executando infer√™ncia...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1b032dc53a74543adc17236f186570a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "      :   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-26 01:43:36.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1m‚úÖ Conclu√≠do! F1-Score: 0.7623\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:36.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 31/36] Re-executando (erro anterior): lucianosb/boto-9B-it | zero_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:36.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 31/36] Executando: lucianosb/boto-9B-it | zero_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:36.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: lucianosb/boto-9B-it | zero_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:36.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:36.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: Fake.Br Corpus\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 7200 amostras\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.111\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1m‚ùå ERRO: Modelo 'lucianosb/boto-9B-it' n√£o encontrado no mapeamento OLLAMA_MODEL_MAPPING. Adicione o mapeamento em config.py ou forne√ßa 'ollama_model' explicitamente.\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 32/36] Re-executando (erro anterior): lucianosb/boto-9B-it | zero_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 32/36] Executando: lucianosb/boto-9B-it | zero_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: lucianosb/boto-9B-it | zero_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:38.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: FakeRecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 52800 amostras\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.898\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1m‚ùå ERRO: Modelo 'lucianosb/boto-9B-it' n√£o encontrado no mapeamento OLLAMA_MODEL_MAPPING. Adicione o mapeamento em config.py ou forne√ßa 'ollama_model' explicitamente.\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 33/36] Re-executando (erro anterior): lucianosb/boto-9B-it | few_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 33/36] Executando: lucianosb/boto-9B-it | few_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: lucianosb/boto-9B-it | few_shot | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:40.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: Fake.Br Corpus\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 7200 amostras\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.826\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1m‚ùå ERRO: Modelo 'lucianosb/boto-9B-it' n√£o encontrado no mapeamento OLLAMA_MODEL_MAPPING. Adicione o mapeamento em config.py ou forne√ßa 'ollama_model' explicitamente.\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 34/36] Re-executando (erro anterior): lucianosb/boto-9B-it | few_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 34/36] Executando: lucianosb/boto-9B-it | few_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: lucianosb/boto-9B-it | few_shot | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:41.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: FakeRecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 52800 amostras\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.853\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1m‚ùå ERRO: Modelo 'lucianosb/boto-9B-it' n√£o encontrado no mapeamento OLLAMA_MODEL_MAPPING. Adicione o mapeamento em config.py ou forne√ßa 'ollama_model' explicitamente.\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 35/36] Re-executando (erro anterior): lucianosb/boto-9B-it | chain_of_thought | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 35/36] Executando: lucianosb/boto-9B-it | chain_of_thought | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: lucianosb/boto-9B-it | chain_of_thought | fakebr\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:44.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: Fake.Br Corpus\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 7200 amostras\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.792\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1m‚ùå ERRO: Modelo 'lucianosb/boto-9B-it' n√£o encontrado no mapeamento OLLAMA_MODEL_MAPPING. Adicione o mapeamento em config.py ou forne√ßa 'ollama_model' explicitamente.\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[Experimento 36/36] Re-executando (erro anterior): lucianosb/boto-9B-it | chain_of_thought | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m[Experimento 36/36] Executando: lucianosb/boto-9B-it | chain_of_thought | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mExperimento: lucianosb/boto-9B-it | chain_of_thought | fakerecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[1/4] Carregando dataset...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:45.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCarregando dataset: FakeRecogna\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:47.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.data_loader\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mDataset carregado: 52800 amostras\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:47.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1m      Amostras: 1000\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:47.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m[2/4] Carregando modelo via Ollama...\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:47.506\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_experiment\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1m‚ùå ERRO: Modelo 'lucianosb/boto-9B-it' n√£o encontrado no mapeamento OLLAMA_MODEL_MAPPING. Adicione o mapeamento em config.py ou forne√ßa 'ollama_model' explicitamente.\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:47.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1m‚úÖ Benchmark conclu√≠do!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Executar todos os experimentos\n",
        "all_results = []\n",
        "results_dir = Path(PATHS['results_dir'])\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "total_experiments = len(MODELS_TO_TEST) * len(STRATEGIES_TO_TEST) * len(DATASETS_TO_TEST)\n",
        "experiment_count = 0\n",
        "\n",
        "logger.info(f\"Iniciando benchmark com {total_experiments} experimentos...\")\n",
        "\n",
        "for model_name in MODELS_TO_TEST:\n",
        "    for strategy in STRATEGIES_TO_TEST:\n",
        "        for dataset_name in DATASETS_TO_TEST:\n",
        "            experiment_count += 1\n",
        "            \n",
        "            filename = f\"{model_name.replace('/', '_')}_{strategy}_{dataset_name}.json\"\n",
        "            filepath = results_dir / filename\n",
        "            \n",
        "            # Verificar se j√° existe resultado de sucesso\n",
        "            if filepath.exists():\n",
        "                try:\n",
        "                    with open(filepath, \"r\") as f:\n",
        "                        existing_results = json.load(f)\n",
        "                        \n",
        "                    if \"error\" not in existing_results:\n",
        "                        logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Pular: {model_name} | {strategy} | {dataset_name} (J√° conclu√≠do)\")\n",
        "                        all_results.append(existing_results)\n",
        "                        continue\n",
        "                    else:\n",
        "                        logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Re-executando (erro anterior): {model_name} | {strategy} | {dataset_name}\")\n",
        "                except (json.JSONDecodeError, Exception):\n",
        "                    logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Re-executando (arquivo corrompido): {model_name} | {strategy} | {dataset_name}\")\n",
        "\n",
        "            logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Executando: {model_name} | {strategy} | {dataset_name}\")\n",
        "            \n",
        "            results = run_experiment(\n",
        "                model_name=model_name,\n",
        "                strategy=strategy,\n",
        "                dataset_name=dataset_name,\n",
        "                sample_size=SAMPLE_SIZE,\n",
        "                seed=SEED\n",
        "            )\n",
        "            \n",
        "            all_results.append(results)\n",
        "            \n",
        "            # Salvar resultados parciais\n",
        "            with open(filepath, \"w\") as f:\n",
        "                json.dump(results, f, indent=2)\n",
        "\n",
        "\n",
        "logger.info(\"‚úÖ Benchmark conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conclus√£o\n",
        "\n",
        "‚úÖ **Benchmark conclu√≠do!**\n",
        "\n",
        "Os resultados foram salvos em arquivos JSON no diret√≥rio `reports/results/`.\n",
        "\n",
        "Para analisar os resultados, execute o notebook `04_benchmark_analysis.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-01-26 01:43:47.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1m‚úÖ Todos os resultados foram salvos em: /Users/joaoroldi/Projects/tcc2/reports\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:47.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m   Total de arquivos: 36\u001b[0m\n",
            "\u001b[32m2026-01-26 01:43:47.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m   Execute o notebook 04_benchmark_analysis.ipynb para analisar os resultados.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Resumo dos resultados salvos\n",
        "logger.info(f\"‚úÖ Todos os resultados foram salvos em: {results_dir}\")\n",
        "logger.info(f\"   Total de arquivos: {len(list(results_dir.glob('*.json')))}\")\n",
        "logger.info(\"   Execute o notebook 04_benchmark_analysis.ipynb para analisar os resultados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
