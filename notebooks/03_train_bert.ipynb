{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmark de Modelos BERT para Detec√ß√£o de Fake News\n",
        "\n",
        "Este notebook implementa um benchmark completo de modelos BERT (fine-tuning) para classificar not√≠cias como verdadeiras ou falsas, utilizando m√∫ltiplos datasets e varia√ß√µes de BERT em portugu√™s.\n",
        "\n",
        "## Objetivos:\n",
        "1. Testar m√∫ltiplos modelos BERT pr√©-treinados em portugu√™s\n",
        "2. Avaliar em m√∫ltiplos datasets (FakeBR e FakeRecogna)\n",
        "3. Comparar desempenho entre diferentes arquiteturas BERT\n",
        "4. Salvar resultados em formato compat√≠vel com o benchmark de LLMs\n",
        "\n",
        "## Modelos BERT a testar:\n",
        "- **BERTimbau Base**: `neuralmind/bert-base-portuguese-cased`\n",
        "- **BERTimbau Large**: `neuralmind/bert-large-portuguese-cased`\n",
        "- **BERT Multilingual**: `bert-base-multilingual-cased` (opcional)\n",
        "\n",
        "‚ö†Ô∏è **Aten√ß√£o**: Este notebook pode levar v√°rias horas para executar, especialmente para o modelo Large!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar path do projeto\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Configurar logger\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\")\n",
        "logger.add(project_root / \"reports/logs/03_train_bert.log\", rotation=\"10 MB\")\n",
        "\n",
        "logger.info(f\"Project root: {project_root}\")\n",
        "\n",
        "# autoreload\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    BertTokenizer, \n",
        "    BertForSequenceClassification, \n",
        "    Trainer, \n",
        "    TrainingArguments, \n",
        "    TrainerCallback,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# M√≥dulos do projeto\n",
        "from config import DATASETS, PATHS, EXPERIMENT_CONFIG\n",
        "from data.data_loader import load_dataset, prepare_test_set\n",
        "from models.metrics import calculate_metrics\n",
        "\n",
        "class SleepCallback(TrainerCallback):\n",
        "    \"\"\"Callback para dormir entre passos e reduzir aquecimento.\"\"\"\n",
        "    def __init__(self, sleep_time=1.0):\n",
        "        self.sleep_time = sleep_time\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        time.sleep(self.sleep_time)\n",
        "\n",
        "# Configura√ß√£o de dispositivo (GPU se dispon√≠vel)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "logger.info(f\"Usando dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configura√ß√£o do Benchmark BERT\n",
        "\n",
        "Definir os modelos BERT a testar e os datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelos BERT a testar\n",
        "BERT_MODELS = [\n",
        "    \"neuralmind/bert-base-portuguese-cased\",      # BERTimbau Base\n",
        "    \"neuralmind/bert-large-portuguese-cased\",     # BERTimbau Large\n",
        "    \"bert-base-multilingual-cased\",               # BERT Multilingual\n",
        "]\n",
        "\n",
        "# Datasets a testar\n",
        "DATASETS_TO_TEST = list(DATASETS.keys())\n",
        "\n",
        "# Configura√ß√µes de treinamento\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_epochs\": 3,\n",
        "    \"batch_size\": 8,\n",
        "    \"eval_batch_size\": 16,\n",
        "    \"max_length\": 512,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"seed\": EXPERIMENT_CONFIG[\"random_seed\"],\n",
        "    \"test_size\": 0.2,\n",
        "}\n",
        "\n",
        "# Tamanho da amostra para teste (usar None para usar todo o dataset)\n",
        "SAMPLE_SIZE = EXPERIMENT_CONFIG[\"test_sample_size\"]\n",
        "\n",
        "logger.info(f\"Modelos BERT: {len(BERT_MODELS)}\")\n",
        "logger.info(f\"Datasets: {len(DATASETS_TO_TEST)}\")\n",
        "logger.info(f\"Total de experimentos: {len(BERT_MODELS) * len(DATASETS_TO_TEST)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Classe Dataset e Fun√ß√µes Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FakeNewsDataset(Dataset):\n",
        "    \"\"\"Dataset para classifica√ß√£o de fake news com BERT.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def prepare_datasets(df, tokenizer, test_size=0.2, seed=42, max_len=512):\n",
        "    \"\"\"Prepara datasets de treino e valida√ß√£o.\"\"\"\n",
        "    # Divis√£o Treino/Teste\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        df['text'].tolist(), \n",
        "        df['label'].tolist(), \n",
        "        test_size=test_size, \n",
        "        random_state=seed,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "    \n",
        "    train_dataset = FakeNewsDataset(train_texts, train_labels, tokenizer, max_len=max_len)\n",
        "    val_dataset = FakeNewsDataset(val_texts, val_labels, tokenizer, max_len=max_len)\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"Fun√ß√£o para calcular m√©tricas durante o treinamento.\"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    \n",
        "    # Usar a fun√ß√£o do m√≥dulo metrics\n",
        "    metrics_dict = calculate_metrics(labels.tolist(), preds.tolist())\n",
        "    \n",
        "    return {\n",
        "        'accuracy': metrics_dict['accuracy'],\n",
        "        'f1': metrics_dict['f1_score'],\n",
        "        'precision': metrics_dict['precision'],\n",
        "        'recall': metrics_dict['recall'],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Fun√ß√£o de Execu√ß√£o de Experimento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Executar Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_bert_experiment(model_name, dataset_name, sample_size, seed, config):\n",
        "    \"\"\"Executa um √∫nico experimento BERT.\"\"\"\n",
        "    logger.info(f\"Experimento: {model_name} | {dataset_name}\")\n",
        "    \n",
        "    try:\n",
        "        # 1. Carregar dados\n",
        "        logger.info(\"[1/5] Carregando dataset...\")\n",
        "        df = load_dataset(dataset_name)\n",
        "        \n",
        "        # Preparar conjunto de teste e treino\n",
        "        if sample_size and sample_size < len(df):\n",
        "            # Separar teste primeiro usando √≠ndices originais\n",
        "            if sample_size // 2 <= len(df[df[\"label\"] == 1]) and sample_size // 2 <= len(df[df[\"label\"] == 0]):\n",
        "                # Amostragem balanceada\n",
        "                samples_per_class = sample_size // 2\n",
        "                df_fake = df[df[\"label\"] == 1].sample(n=samples_per_class, random_state=seed)\n",
        "                df_true = df[df[\"label\"] == 0].sample(n=samples_per_class, random_state=seed)\n",
        "                test_df = pd.concat([df_fake, df_true]).sample(frac=1, random_state=seed)\n",
        "                test_indices = test_df.index.tolist()\n",
        "            else:\n",
        "                # Amostragem simples se n√£o houver amostras suficientes\n",
        "                test_df = df.sample(n=min(sample_size, len(df)), random_state=seed)\n",
        "                test_indices = test_df.index.tolist()\n",
        "            \n",
        "            # Usar o resto para treino\n",
        "            train_df = df.drop(test_indices).reset_index(drop=True)\n",
        "            test_df = test_df.reset_index(drop=True)\n",
        "        else:\n",
        "            # Usar todo o dataset, dividindo em treino/teste\n",
        "            train_df, test_df = train_test_split(\n",
        "                df, \n",
        "                test_size=config[\"test_size\"], \n",
        "                random_state=seed,\n",
        "                stratify=df['label']\n",
        "            )\n",
        "            test_df = test_df.reset_index(drop=True)\n",
        "            train_df = train_df.reset_index(drop=True)\n",
        "        \n",
        "        logger.info(f\"      Treino: {len(train_df)} amostras\")\n",
        "        logger.info(f\"      Teste: {len(test_df)} amostras\")\n",
        "        \n",
        "        # 2. Carregar tokenizer e modelo\n",
        "        logger.info(\"[2/5] Carregando modelo e tokenizer...\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        except:\n",
        "            # Fallback para BertTokenizer se AutoTokenizer falhar\n",
        "            tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        try:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                model_name, \n",
        "                num_labels=2\n",
        "            )\n",
        "        except:\n",
        "            # Fallback para BertForSequenceClassification\n",
        "            model = BertForSequenceClassification.from_pretrained(\n",
        "                model_name, \n",
        "                num_labels=2\n",
        "            )\n",
        "        \n",
        "        model = model.to(device)\n",
        "        logger.info(f\"      Modelo carregado: {model_name}\")\n",
        "        \n",
        "        # 3. Preparar datasets\n",
        "        logger.info(\"[3/5] Preparando datasets...\")\n",
        "        train_dataset, val_dataset = prepare_datasets(\n",
        "            train_df, \n",
        "            tokenizer, \n",
        "            test_size=0.2,  # 20% do treino para valida√ß√£o\n",
        "            seed=seed,\n",
        "            max_len=config[\"max_length\"]\n",
        "        )\n",
        "        \n",
        "        # 4. Configurar treinamento\n",
        "        logger.info(\"[4/5] Configurando treinamento...\")\n",
        "        output_dir = f\"./results/bert_{model_name.replace('/', '_')}_{dataset_name}\"\n",
        "        \n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=config[\"num_epochs\"],\n",
        "            per_device_train_batch_size=config[\"batch_size\"],\n",
        "            per_device_eval_batch_size=config[\"eval_batch_size\"],\n",
        "            warmup_steps=config[\"warmup_steps\"],\n",
        "            weight_decay=config[\"weight_decay\"],\n",
        "            learning_rate=config[\"learning_rate\"],\n",
        "            logging_dir=f'{output_dir}/logs',\n",
        "            logging_steps=50,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=100,\n",
        "            save_total_limit=2,\n",
        "            load_best_model_at_end=True,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=100,\n",
        "            seed=seed,\n",
        "        )\n",
        "        \n",
        "        # 5. Treinar modelo\n",
        "        logger.info(\"[5/5] Treinando modelo...\")\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[SleepCallback(sleep_time=0.5)]\n",
        "        )\n",
        "        \n",
        "        # Verificar checkpoints\n",
        "        last_checkpoint = None\n",
        "        if os.path.isdir(training_args.output_dir):\n",
        "            checkpoints = [\n",
        "                os.path.join(training_args.output_dir, d) \n",
        "                for d in os.listdir(training_args.output_dir) \n",
        "                if d.startswith(\"checkpoint-\")\n",
        "            ]\n",
        "            if checkpoints:\n",
        "                last_checkpoint = max(checkpoints, key=os.path.getctime)\n",
        "                logger.info(f\"      Retomando do checkpoint: {last_checkpoint}\")\n",
        "        \n",
        "        # Treinar\n",
        "        train_start = time.time()\n",
        "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "        train_time = time.time() - train_start\n",
        "        \n",
        "        # 6. Avaliar no conjunto de teste\n",
        "        logger.info(\"      Avaliando no conjunto de teste...\")\n",
        "        test_dataset = FakeNewsDataset(\n",
        "            test_df['text'].tolist(),\n",
        "            test_df['label'].tolist(),\n",
        "            tokenizer,\n",
        "            max_len=config[\"max_length\"]\n",
        "        )\n",
        "        \n",
        "        eval_start = time.time()\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        eval_time = time.time() - eval_start\n",
        "        \n",
        "        preds = predictions.predictions.argmax(-1)\n",
        "        true_labels = test_df['label'].tolist()\n",
        "        \n",
        "        # Calcular m√©tricas\n",
        "        metrics = calculate_metrics(true_labels, preds.tolist())\n",
        "        metrics[\"avg_inference_time\"] = eval_time / len(test_df)\n",
        "        metrics[\"total_inference_time\"] = eval_time\n",
        "        metrics[\"training_time\"] = train_time\n",
        "        \n",
        "        # Resultados\n",
        "        results = {\n",
        "            \"model\": model_name,\n",
        "            \"strategy\": \"fine_tuned\",  # BERT usa fine-tuning, n√£o prompting\n",
        "            \"dataset\": dataset_name,\n",
        "            \"sample_size\": len(test_df),\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"metrics\": metrics,\n",
        "            \"training_config\": config,\n",
        "        }\n",
        "        \n",
        "        # Limpar mem√≥ria\n",
        "        del model\n",
        "        del trainer\n",
        "        del tokenizer\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "        \n",
        "        logger.info(f\"‚úÖ Conclu√≠do! F1-Score: {metrics['f1_score']:.4f}\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå ERRO: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"strategy\": \"fine_tuned\",\n",
        "            \"dataset\": dataset_name,\n",
        "            \"error\": str(e)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executar todos os experimentos\n",
        "all_results = []\n",
        "results_dir = Path(PATHS['results_dir'])\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "total_experiments = len(BERT_MODELS) * len(DATASETS_TO_TEST)\n",
        "experiment_count = 0\n",
        "\n",
        "logger.info(f\"Iniciando benchmark BERT com {total_experiments} experimentos...\")\n",
        "\n",
        "for model_name in BERT_MODELS:\n",
        "    for dataset_name in DATASETS_TO_TEST:\n",
        "        experiment_count += 1\n",
        "        \n",
        "        filename = f\"{model_name.replace('/', '_')}_fine_tuned_{dataset_name}.json\"\n",
        "        filepath = results_dir / filename\n",
        "        \n",
        "        # Verificar se j√° existe resultado de sucesso\n",
        "        if filepath.exists():\n",
        "            try:\n",
        "                with open(filepath, \"r\") as f:\n",
        "                    existing_results = json.load(f)\n",
        "                    \n",
        "                if \"error\" not in existing_results:\n",
        "                    logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Pular: {model_name} | {dataset_name} (J√° conclu√≠do)\")\n",
        "                    all_results.append(existing_results)\n",
        "                    continue\n",
        "                else:\n",
        "                    logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Re-executando (erro anterior): {model_name} | {dataset_name}\")\n",
        "            except (json.JSONDecodeError, Exception):\n",
        "                logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Re-executando (arquivo corrompido): {model_name} | {dataset_name}\")\n",
        "\n",
        "        logger.info(f\"[Experimento {experiment_count}/{total_experiments}] Executando: {model_name} | {dataset_name}\")\n",
        "        \n",
        "        results = run_bert_experiment(\n",
        "            model_name=model_name,\n",
        "            dataset_name=dataset_name,\n",
        "            sample_size=SAMPLE_SIZE,\n",
        "            seed=TRAINING_CONFIG[\"seed\"],\n",
        "            config=TRAINING_CONFIG\n",
        "        )\n",
        "        \n",
        "        all_results.append(results)\n",
        "        \n",
        "        # Salvar resultados parciais\n",
        "        with open(filepath, \"w\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "logger.info(\"‚úÖ Benchmark BERT conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Resumo dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo dos resultados\n",
        "logger.info(f\"‚úÖ Todos os resultados foram salvos em: {results_dir}\")\n",
        "logger.info(f\"   Total de arquivos: {len(list(results_dir.glob('*fine_tuned*.json')))}\")\n",
        "\n",
        "# Criar DataFrame com resultados\n",
        "results_data = []\n",
        "for result in all_results:\n",
        "    if \"error\" not in result:\n",
        "        metrics = result[\"metrics\"]\n",
        "        results_data.append({\n",
        "            \"model\": result[\"model\"],\n",
        "            \"dataset\": result[\"dataset\"],\n",
        "            \"accuracy\": metrics[\"accuracy\"],\n",
        "            \"precision\": metrics[\"precision\"],\n",
        "            \"recall\": metrics[\"recall\"],\n",
        "            \"f1_score\": metrics[\"f1_score\"],\n",
        "            \"training_time\": metrics.get(\"training_time\"),\n",
        "            \"avg_inference_time\": metrics.get(\"avg_inference_time\"),\n",
        "        })\n",
        "    else:\n",
        "        results_data.append({\n",
        "            \"model\": result[\"model\"],\n",
        "            \"dataset\": result[\"dataset\"],\n",
        "            \"accuracy\": None,\n",
        "            \"precision\": None,\n",
        "            \"recall\": None,\n",
        "            \"f1_score\": None,\n",
        "            \"error\": result[\"error\"],\n",
        "        })\n",
        "\n",
        "if results_data:\n",
        "    df_results = pd.DataFrame(results_data)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESUMO DOS RESULTADOS\")\n",
        "    print(\"=\"*80)\n",
        "    print(df_results.to_string(index=False))\n",
        "    \n",
        "    # Melhor resultado\n",
        "    if not df_results[\"f1_score\"].isna().all():\n",
        "        best_idx = df_results[\"f1_score\"].idxmax()\n",
        "        best = df_results.loc[best_idx]\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üèÜ MELHOR RESULTADO:\")\n",
        "        print(f\"   Modelo: {best['model']}\")\n",
        "        print(f\"   Dataset: {best['dataset']}\")\n",
        "        print(f\"   F1-Score: {best['f1_score']:.4f}\")\n",
        "        print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclus√£o\n",
        "\n",
        "‚úÖ **Benchmark BERT conclu√≠do!**\n",
        "\n",
        "Os resultados foram salvos em arquivos JSON no diret√≥rio `reports/` no formato:\n",
        "`{model_name}_fine_tuned_{dataset_name}.json`\n",
        "\n",
        "Os resultados s√£o compat√≠veis com o formato do benchmark de LLMs e podem ser analisados junto com os outros resultados usando o notebook `04_benchmark_analysis.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
